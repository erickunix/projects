*/////////////////////////////;
* LOAD DATASET
*/////////////////////////////;

/* Create a library - get access to the data;*/
libname b "C:\Users\Jose Caloca\Desktop";

/* set formats;*/

PROC FORMAT lib=work;
value sex
1 = 'Male'
2 = 'Female'
9 = 'No answer' .d = 'No answer';
value sourceinc
1 = 'Wages or salaries'  
2 = 'Income from self-employment (excluding farming)'  
3 = 'Income from farming'  ยบ
4 = 'Pensions'  
5 = 'Unemployment/redundancy benefit'  
6 = 'Any other social benefits or grants'  
7 = 'Income from investments, savings etc.'  
8 = 'Income from other sources'  
77 = 'Refusal' .b = 'Refusal'  
88 = 'Don''t know' .c = 'Don''t know'  
99 = 'No answer' .d = 'No answer' ;
value mainactivity
1 = 'Employed'
2 = 'Unemployed'
3 = 'Inactive'
4 = 'Other';
value peoplelivinghouse
77 = 'Refusal' .b = 'Refusal'  
88 = 'Don''t know' .c = 'Don''t know'  
99 = 'No answer' .d = 'No answer' ;
value yearsedu
77 = 'Refusal' .b = 'Refusal'  
88 = 'Don''t know' .c = 'Don''t know'  
99 = 'No answer' .d = 'No answer' ;
value age
999 = 'Not available' .d = 'Not available' ;
value national_income
1 = 'Labour Income'
2 = 'Capital Income'
3 = 'Grants'
4 = 'Other Income';
run;

/* load dataset;*/

data ess;
set b.ess9e03_1;
format hincsrca sourceinc. gndr sex. mnactic  hhmmb peoplelivinghouse. eduyrs yearsedu. agea age.;
keep hincsrca gndr mnactic hhmmb eduyrs agea;
where cntry = 'ES';
run;


*/////////////////////////////;
* DROPPING: not applicable, refusal, don't know
and no answer ;
*/////////////////////////////;

*Check the distribution of the response variable PRIOR MODIFYING;

proc freq data=ess;
    table hincsrca;
run;	data ess_01;
set ess;
if hincsrca in (77,88,99) then delete;
if gndr = 9 then delete;
if mnactic in (66,77,88,99) then delete;
if hhmmb in (77,88,99) then delete;
if eduyrs in (77,88,99) then delete;
if agea = 999 then delete;
run;

* Check missing values;

proc means data=ess_01 n nmiss;
var hincsrca gndr mnactic hhmmb eduyrs agea;
run;

*Check the distribution of the response variable AFTER MODIFYING;

proc freq data=ess_01;
    table hincsrca;
run;
*/////////////////////////////;
* TARGET VARIABLE PREPARATION ;
*/////////////////////////////;


/* relabel hincsrca, mnactic*/
data ess_02 (drop=hincsrca);
set ess_01;
format y national_income. mnactic mainactivity.;
if hincsrca in (1, 3) then y=1;
else if hincsrca in (2, 7) then y=2;
else if hincsrca in (4, 5, 6) then y=3;
else if hincsrca=8 then y=4;
else y=.;
if mnactic in (1, 7) then mnactic=1;
else if mnactic in (3, 4) then mnactic=2;
else if mnactic in (5, 6) then mnactic=3;
else mnactic=4;
run;

*Check the distribution of the response variable AFTER MODIFYING;

proc freq data=ess_02;
    table y;
run;

*/////////////////////////////;
* EXPLORATORY DATA ANALYSIS ;
*/////////////////////////////;
/*DISCRIMINATORY PERFORMANCE ANALYSIS;

/*Folder to save the plots*/
%let graphs = C:\Users\Jose Caloca\Desktop;

/*Bar Plot of the hincsrca variable */
ods listing gpath="&graphs";
ods graphics /
imagename="hincsrca_barplot"
imagefmt=png;

proc SGPLOT data = ess_01;
vbar hincsrca / datalabel 
categoryorder=respdesc;
xaxis display=(nolabel);
yaxis grid ;
run;
quit;
ods close;

/*Bar Plot of the Target variable */
ods listing gpath="&graphs";
ods graphics /
imagename="source_of_income_barplot"
imagefmt=png;	proc SGPLOT data = ess_02;
vbar y / datalabel 
categoryorder=respdesc;
xaxis display=(nolabel);
yaxis grid ;
run;
quit;
ods close;

/* Categorical predictors;*/

%macro Frequency(Var);
    proc freq data=ess_02;
        tables &Var.*y;
        ods output CrossTabFreqs=pct01;
    run;
    ods listing gpath="&graphs";
    ods graphics /
    imagename="&Var._barplot"
    imagefmt=png;
    proc sgplot data=pct01(where=(^missing(RowPercent)));
        vbar &Var. / group=y groupdisplay=cluster response=RowPercent datalabel categoryorder=respdesc;
    run;
%mend;
%Frequency(gndr);
%Frequency(mnactic);
/* Continuous predictors;*/

%macro Continuous(Var);
    ods listing gpath="&graphs";
    ods graphics /
    imagename="&Var._barplot"
    imagefmt=png;
    proc sgplot data=ess_02; 
    vbar &Var. / group=y;
    run;
%mend;
%Continuous(hincsrca); *target variable;
%Continuous(hhmmb);
%Continuous(eduyrs);
%Continuous(agea);

/******* DISTRIBUTION ANALYSIS;

/* Statistical outputs for all varables */
proc univariate data=ess_02 plots;
var gndr mnactic hhmmb eduyrs agea;
histogram;
run;
*/////////////////////////////;
* COLLINEARITY ;
*/////////////////////////////;

*correlation matrix numerical variables;
proc corr data=ess_02;
 var agea hhmmb eduyrs;
run;
*chi-square test categorical variables;
proc freq data=ess_02;
tables gndr*mnactic/ chisq;
run;

*/////////////////////////////;
* MODELLING ;
*/////////////////////////////;

proc logistic data=ess_02;
class  gndr (param=ref ref='Male') mnactic (param=ref ref='Employed');
model y (ref='Labour Income') = agea gndr mnactic hhmmb eduyrs / 
link=glogit expb rsquare aggregate scale=none;
output out=out predicted=p;
run;

proc sgplot data=out;
scatter x=agea y=p / group=_LEVEL_;
run;


#PYTHON RF CODE

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from pprint import pprint
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
from sklearn import metrics
import matplotlib.pyplot as plt

dataset = pd.read_sas("ess_02.sas7bdat")
dataset.info()

dataset['gndr'] = dataset.gndr.astype('object')
dataset['mnactic'] = dataset.mnactic.astype('object')

#First: we create two data sets for numeric and non-numeric data
numerical = dataset.select_dtypes(exclude=['object'])
categorical = dataset.select_dtypes(include=['object'])

#Second: One-hot encode the non-numeric columns
z = pd.get_dummies(categorical)

#Third: Union the one-hot encoded columns to the numeric ones
df = pd.concat([numerical, onehot], axis=1)

# We create the X and y data sets
X = df.loc[ : , df.columns != 'y']
y = df[['y']]

# Create training, evaluation and test sets
X_train, test_X, y_train, test_y = train_test_split(X, y, test_size=.3, random_state=123)

# percentage of the classes in the training set
round(y_train['y'].value_counts()*100/len(y_train['y']), 2)

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(2, 10, num = 8)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)

		
# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestRegressor()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X_train, y_train)
#We can view the best parameters from fitting the random search:
rf_random.best_params_
# we make predictions
best_random = rf_random.best_estimator_
predictions = pd.DataFrame(best_random.predict(test_X))
#We calculate the AUC
fpr, tpr, thresholds = metrics.roc_curve(test_y, predictions, pos_label=3)
metrics.auc(fpr, tpr)
# get importance
importance = best_random.feature_importances_
# summarize feature importance
var_importance = pd.DataFrame({'col_name': best_random.feature_importances_}, index=X_train.columns).sort_values(by='col_name', ascending=False)
# plot feature importance
importance = pd.DataFrame({'col_name': best_random.feature_importances_})
index = np.array(X_train.columns)
#index= np.arange(len(X_train.columns))
plt.figure(figsize=(10, 5))
colors = plt.cm.BuPu(np.linspace(0.2, 0.7, len(importance)))
plt.xticks(rotation=90)
plt.bar(index, importance['col_name'],  color=colors)
plt.grid(color='#95a5a6', linestyle='--', linewidth=1, axis='y', alpha=0.7)
plt.title('Importance')
plt.show()	
